{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 2: arXiv API Integration & PDF Processing\n",
    "\n",
    "**What We're Building This Week:**\n",
    "\n",
    "Week 2 focuses on implementing the core data ingestion pipeline that will automatically fetch, process, and store arXiv papers. This is the foundation that feeds our RAG system with fresh academic content.\n",
    "\n",
    "## Week 2 Focus Areas\n",
    "\n",
    "### ðŸŽ¯ Core Objectives\n",
    "- **arXiv API Integration**: Build a robust client with rate limiting and retry logic\n",
    "- **PDF Processing Pipeline**: Download and parse scientific PDFs with structured content extraction\n",
    "- **Database Storage**: Persist paper metadata and content in PostgreSQL\n",
    "- **Error Handling**: Implement comprehensive error handling and graceful degradation\n",
    "- **Automation Ready**: Prepare components for Airflow orchestration\n",
    "\n",
    "### ðŸ”§ What We'll Test In This Notebook\n",
    "1. **arXiv API Client** - Fetch CS.AI papers with proper rate limiting\n",
    "2. **PDF Download System** - Download and cache PDFs with error handling  \n",
    "3. **Docling PDF Parser** - Extract structured content (sections, tables, figures)\n",
    "4. **Database Integration** - Store and retrieve papers from PostgreSQL\n",
    "5. **Complete Pipeline** - End-to-end processing from arXiv to database\n",
    "6. **Production Readiness** - Error handling, logging, and performance metrics\n",
    "\n",
    "\n",
    "### ðŸ“Š Success Metrics\n",
    "- arXiv API calls succeed with proper rate limiting\n",
    "- PDF download and caching works reliably  \n",
    "- Docling extracts structured content from scientific PDFs\n",
    "- Database stores complete paper metadata\n",
    "- Pipeline handles errors gracefully and continues processing\n",
    "- All components ready for Airflow automation (Week 2+)\n",
    "\n",
    "---\n",
    "\n",
    "## Week 2 Component Status\n",
    "| Component | Purpose | Status |\n",
    "|-----------|---------|--------|\n",
    "| **arXiv API Client** | Fetch CS.AI papers with rate limiting | âœ… Complete |\n",
    "| **PDF Downloader** | Download and cache PDFs locally | âœ… Complete |\n",
    "| **Docling Parser** | Extract structured content from PDFs | âœ… Complete |\n",
    "| **Metadata Fetcher** | Orchestrate complete pipeline | âœ… Complete |\n",
    "| **Database Storage** | Store papers in PostgreSQL | âš ï¸ Needs volume refresh |\n",
    "| **Airflow DAGs** | Automate daily ingestion | âš ï¸ Needs container update |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## âš ï¸ IMPORTANT: Week 2 Database Schema Update\n",
    "\n",
    "**NEW USERS OR SCHEMA CONFLICTS**: If you're starting Week 2 fresh or experiencing database schema conflicts, use this clean start approach:\n",
    "\n",
    "### Fresh Start (Recommended for Week 2)\n",
    "```bash\n",
    "# Complete clean slate - removes all data but ensures correct schema\n",
    "docker compose down -v\n",
    "\n",
    "# Build fresh containers with latest code\n",
    "docker compose up --build -d\n",
    "```\n",
    "\n",
    "**When to use this:**\n",
    "- First time running Week 2 \n",
    "- Schema errors or column missing errors\n",
    "- Want to start with clean database\n",
    "- Previous Week 1 data not important\n",
    "\n",
    "**Note**: This destroys existing data but ensures you have the correct Week 2 schema with all new columns for PDF processing and arXiv metadata.\n",
    "\n",
    "---\n",
    "\n",
    "## Prerequisites Check\n",
    "\n",
    "**Before starting:**\n",
    "1. Week 1 infrastructure completed\n",
    "2. UV environment activated\n",
    "3. Docker Desktop running\n",
    "\n",
    "**Why fresh containers?** Week 2 includes new Airflow dependencies and code changes that require rebuilding images rather than using cached layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WEEK 2 CONTAINER & SERVICE HEALTH CHECK\n",
      "==================================================\n",
      "Project root: d:\\Projects\\Agentic_RAG\\arxiv-paper-curator\n",
      "\n",
      "1. Checking container status...\n",
      "âœ“ Containers are running:\n",
      "   NAME             IMAGE                                            COMMAND                  SERVICE                 CREATED       STATUS                 PORTS\n",
      "   rag-airflow      arxiv-paper-curator-airflow                      \"/entrypoint.sh\"         airflow                 4 hours ago   Up 4 hours (healthy)   0.0.0.0:8080->8080/tcp, [::]:8080->8080/tcp\n",
      "   rag-api          arxiv-paper-curator-api                          \"uvicorn src.main:apÃ¢â‚¬Â¦\"   api                     4 hours ago   Up 4 hours (healthy)   0.0.0.0:8000->8000/tcp, [::]:8000->8000/tcp\n",
      "   rag-dashboards   opensearchproject/opensearch-dashboards:2.19.0   \"./opensearch-dashboÃ¢â‚¬Â¦\"   opensearch-dashboards   4 hours ago   Up 4 hours (healthy)   0.0.0.0:5601->5601/tcp, [::]:5601->5601/tcp\n",
      "   rag-ollama       ollama/ollama:0.11.2                             \"/bin/ollama serve\"      ollama                  4 hours ago   Up 4 hours (healthy)   0.0.0.0:11434->11434/tcp, [::]:11434->11434/tcp\n",
      "   rag-opensearch   opensearchproject/opensearch:2.19.0              \"./opensearch-dockerÃ¢â‚¬Â¦\"   opensearch              4 hours ago   Up 4 hours (healthy)   0.0.0.0:9200->9200/tcp, [::]:9200->9200/tcp, 0.0.0.0:9600->9600/tcp, [::]:9600->9600/tcp\n",
      "   rag-postgres     postgres:16-alpine                               \"docker-entrypoint.sÃ¢â‚¬Â¦\"   postgres                4 hours ago   Up 4 hours (healthy)   0.0.0.0:5432->5432/tcp, [::]:5432->5432/tcp\n",
      "\n",
      "2. Checking service health...\n",
      "âœ“ FastAPI: Healthy\n",
      "âœ“ PostgreSQL (via API): Healthy\n",
      "âœ“ Ollama: Healthy\n",
      "âœ“ OpenSearch: Healthy\n",
      "âœ“ Airflow: Healthy\n",
      "\n",
      "==================================================\n",
      "âœ“ ALL SERVICES HEALTHY! Ready for Week 2 development.\n"
     ]
    }
   ],
   "source": [
    "# Check if Fresh Containers are Built and All Services Healthy\n",
    "import subprocess\n",
    "import requests\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"WEEK 2 CONTAINER & SERVICE HEALTH CHECK\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Find project root\n",
    "current_dir = Path.cwd()\n",
    "if current_dir.name == \"week2\" and current_dir.parent.name == \"notebooks\":\n",
    "    project_root = current_dir.parent.parent\n",
    "elif (current_dir / \"compose.yml\").exists():\n",
    "    project_root = current_dir\n",
    "else:\n",
    "    print(\"âœ— Could not find project root\")\n",
    "    exit()\n",
    "\n",
    "print(f\"Project root: {project_root}\")\n",
    "\n",
    "# Step 1: Check if containers are built and running\n",
    "print(\"\\n1. Checking container status...\")\n",
    "try:\n",
    "    result = subprocess.run(\n",
    "        [\"docker\", \"compose\", \"ps\", \"--format\", \"table\"],\n",
    "        cwd=str(project_root),\n",
    "        capture_output=True,\n",
    "        text=True,\n",
    "        timeout=10\n",
    "    )\n",
    "    \n",
    "    if result.returncode == 0 and result.stdout.strip():\n",
    "        print(\"âœ“ Containers are running:\")\n",
    "        for line in result.stdout.strip().split('\\n'):\n",
    "            print(f\"   {line}\")\n",
    "    else:\n",
    "        print(\"âœ— No containers running or docker compose failed\")\n",
    "        print(\"Please run the build commands from the markdown cell above\")\n",
    "        exit()\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"âœ— Error checking containers: {e}\")\n",
    "    print(\"Please run the build commands from the markdown cell above\")\n",
    "    exit()\n",
    "\n",
    "# Step 2: Check all service health (corrected endpoints)\n",
    "print(\"\\n2. Checking service health...\")\n",
    "services_to_test = {\n",
    "    \"FastAPI\": \"http://localhost:8000/api/v1/health\",\n",
    "    \"PostgreSQL (via API)\": \"http://localhost:8000/api/v1/health\", \n",
    "    \"Ollama\": \"http://localhost:11434/api/version\",\n",
    "    \"OpenSearch\": \"http://localhost:9200/_cluster/health\",\n",
    "    \"Airflow\": \"http://localhost:8080/health\"\n",
    "}\n",
    "\n",
    "all_healthy = True\n",
    "for service_name, url in services_to_test.items():\n",
    "    try:\n",
    "        response = requests.get(url, timeout=5)\n",
    "        if response.status_code == 200:\n",
    "            print(f\"âœ“ {service_name}: Healthy\")\n",
    "        else:\n",
    "            print(f\"âœ— {service_name}: HTTP {response.status_code}\")\n",
    "            all_healthy = False\n",
    "    except requests.exceptions.ConnectionError:\n",
    "        print(f\"âœ— {service_name}: Not accessible\")\n",
    "        all_healthy = False\n",
    "    except Exception as e:\n",
    "        print(f\"âœ— {service_name}: {type(e).__name__}\")\n",
    "        all_healthy = False\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "if all_healthy:\n",
    "    print(\"âœ“ ALL SERVICES HEALTHY! Ready for Week 2 development.\")\n",
    "else:\n",
    "    print(\"âœ— Some services need attention.\")\n",
    "    print(\"If you just rebuilt containers, wait 1-2 minutes and run this cell again.\")\n",
    "    print(\"Airflow and OpenSearch take longest to start up.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python Version: 3.12.12\n",
      "Environment: d:\\Projects\\Agentic_RAG\\arxiv-paper-curator\\.venv\\Scripts\\python.exe\n",
      "âœ“ Project root: d:\\Projects\\Agentic_RAG\\arxiv-paper-curator\n"
     ]
    }
   ],
   "source": [
    "# Environment Check\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "print(f\"Python Version: {sys.version_info.major}.{sys.version_info.minor}.{sys.version_info.micro}\")\n",
    "print(f\"Environment: {sys.executable}\")\n",
    "\n",
    "# Find project root\n",
    "current_dir = Path.cwd()\n",
    "if current_dir.name == \"week2\" and current_dir.parent.name == \"notebooks\":\n",
    "    project_root = current_dir.parent.parent\n",
    "elif (current_dir / \"compose.yml\").exists():\n",
    "    project_root = current_dir\n",
    "else:\n",
    "    project_root = None\n",
    "\n",
    "if project_root and (project_root / \"compose.yml\").exists():\n",
    "    print(f\"âœ“ Project root: {project_root}\")\n",
    "    # Add project to Python path\n",
    "    sys.path.insert(0, str(project_root))\n",
    "else:\n",
    "    print(\"âœ— Missing compose.yml - check directory\")\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Service Health Verification\n",
    "\n",
    "Ensure all services from Week 1 are still running correctly:\n",
    "\n",
    "### ðŸ”— Service Access Points\n",
    "- **FastAPI**: http://localhost:8000/docs (API documentation)\n",
    "- **PostgreSQL**: via API or `docker exec -it rag-postgres psql -U rag_user -d rag_db`\n",
    "- **OpenSearch**: http://localhost:9200/_cluster/health\n",
    "- **Ollama**: http://localhost:11434 (LLM service)\n",
    "- **Airflow**: http://localhost:8080 (Username: `admin`, Password: `admin`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WEEK 2 PREREQUISITE CHECK\n",
      "==================================================\n",
      "âœ“ FastAPI: Healthy\n",
      "âœ“ PostgreSQL (via API): Healthy\n",
      "âœ“ Ollama: Healthy\n",
      "âœ“ OpenSearch: Healthy\n",
      "âœ“ Airflow: Healthy\n",
      "\n",
      "All services healthy! Ready for Week 2 development.\n"
     ]
    }
   ],
   "source": [
    "# Test Service Connectivity\n",
    "import requests\n",
    "import subprocess\n",
    "import json\n",
    "\n",
    "services_to_test = {\n",
    "    \"FastAPI\": \"http://localhost:8000/api/v1/health\",\n",
    "    \"PostgreSQL (via API)\": \"http://localhost:8000/api/v1/health\", \n",
    "    \"Ollama\": \"http://localhost:11434/api/version\",\n",
    "    \"OpenSearch\": \"http://localhost:9200/_cluster/health\",\n",
    "    \"Airflow\": \"http://localhost:8080/health\"  \n",
    "}\n",
    "\n",
    "print(\"WEEK 2 PREREQUISITE CHECK\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "all_healthy = True\n",
    "\n",
    "for service_name, url in services_to_test.items():\n",
    "    try:\n",
    "        response = requests.get(url, timeout=5)\n",
    "        if response.status_code == 200:\n",
    "            print(f\"âœ“ {service_name}: Healthy\")\n",
    "        else:\n",
    "            print(f\"âœ— {service_name}: HTTP {response.status_code}\")\n",
    "            all_healthy = False\n",
    "    except requests.exceptions.ConnectionError:\n",
    "        print(f\"âœ— {service_name}: Not accessible\")\n",
    "        all_healthy = False\n",
    "    except Exception as e:\n",
    "        print(f\"âœ— {service_name}: {type(e).__name__}\")\n",
    "        all_healthy = False\n",
    "\n",
    "print()\n",
    "if all_healthy:\n",
    "    print(\"All services healthy! Ready for Week 2 development.\")\n",
    "else:\n",
    "    print(\"Some services need attention. Check Week 1 notebook.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. arXiv API Client Testing\n",
    "\n",
    "Test the arXiv API client with rate limiting and retry logic:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TESTING ARXIV API CLIENT\n",
      "========================================\n",
      "âœ“ Client created: https://export.arxiv.org/api/query\n",
      "   Rate limit: 3.0s\n",
      "   Max results: 100\n",
      "   Category: cs.AI\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test arXiv API Client\n",
    "import asyncio\n",
    "from datetime import datetime, timedelta\n",
    "import importlib\n",
    "import os\n",
    "\n",
    "# Debug: Check what the OLLAMA_MODELS environment variable is set to\n",
    "if 'OLLAMA_MODELS' in os.environ:\n",
    "    print(f\"DEBUG: OLLAMA_MODELS = '{os.environ['OLLAMA_MODELS']}' (repr: {repr(os.environ['OLLAMA_MODELS'])})\")\n",
    "    # Delete it completely to avoid JSON parsing issues\n",
    "    del os.environ['OLLAMA_MODELS']\n",
    "    print(\"Cleared OLLAMA_MODELS environment variable\")\n",
    "\n",
    "# Reload config module to get the latest changes\n",
    "import src.config\n",
    "importlib.reload(src.config)\n",
    "\n",
    "# Import our arXiv client\n",
    "from src.services.arxiv.factory import make_arxiv_client\n",
    "\n",
    "print(\"\\nTESTING ARXIV API CLIENT\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Create client\n",
    "arxiv_client = make_arxiv_client()\n",
    "print(f\"âœ“ Client created: {arxiv_client.base_url}\")\n",
    "print(f\"   Rate limit: {arxiv_client.rate_limit_delay}s\")\n",
    "print(f\"   Max results: {arxiv_client.max_results}\")\n",
    "print(f\"   Category: {arxiv_client.search_category}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test 1: Fetch Recent CS.AI Papers\n",
      "âœ“ Fetched 2 papers\n",
      "   1. [2601.00791v1] Geometry of Reason: Spectral Signatures of Valid Mathematica...\n",
      "      Authors: Valentin NoÃ«l\n",
      "      Categories: cs.LG, cs.AI, cs.CL, cs.LO\n",
      "      Published: 2026-01-02T18:49:37Z\n",
      "\n",
      "   2. [2601.00785v1] FedHypeVAE: Federated Learning with Hypernetwork Generated C...\n",
      "      Authors: Sunny Gupta, Amit Sethi\n",
      "      Categories: cs.LG, cs.AI, cs.CV\n",
      "      Published: 2026-01-02T18:40:41Z\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test Paper Fetching\n",
    "async def test_paper_fetching():\n",
    "    \"\"\"Test fetching papers from arXiv with rate limiting.\"\"\"\n",
    "    \n",
    "    print(\"Test 1: Fetch Recent CS.AI Papers\")\n",
    "    try:\n",
    "        papers = await arxiv_client.fetch_papers(\n",
    "            max_results=2, \n",
    "            sort_by=\"submittedDate\",\n",
    "            sort_order=\"descending\"\n",
    "        )\n",
    "        \n",
    "        print(f\"âœ“ Fetched {len(papers)} papers\")\n",
    "        \n",
    "        if papers:\n",
    "            for i, paper in enumerate(papers[:2], 1):\n",
    "                print(f\"   {i}. [{paper.arxiv_id}] {paper.title[:60]}...\")\n",
    "                print(f\"      Authors: {', '.join(paper.authors[:2])}{'...' if len(paper.authors) > 2 else ''}\")\n",
    "                print(f\"      Categories: {', '.join(paper.categories)}\")\n",
    "                print(f\"      Published: {paper.published_date}\")\n",
    "                print()\n",
    "        \n",
    "        return papers\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âœ— Error fetching papers: {e}\")\n",
    "        if \"503\" in str(e):\n",
    "            print(\"   arXiv API temporarily unavailable (normal)\")\n",
    "            print(\"   Rate limiting and error handling working correctly\")\n",
    "        return []\n",
    "\n",
    "# Run the test\n",
    "papers = await test_paper_fetching()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test 2: Date Range Filtering\n",
      "âœ“ Date filtering test: 5 papers from 20250808-20250809\n",
      "   1. [2508.07111v1] Investigating Intersectional Bias in Large Language Models u...\n",
      "      Authors: Falaah Arif Khan, Nivedha Sivakumar...\n",
      "      Categories: cs.CL, cs.AI\n",
      "      Published: 2025-08-09T22:24:40Z\n",
      "\n",
      "   2. [2508.07107v2] Designing a Feedback-Driven Decision Support System for Dyna...\n",
      "      Authors: Timothy Oluwapelumi Adeyemi, Nadiah Fahad AlOtaibi\n",
      "      Categories: cs.AI, cs.CY\n",
      "      Published: 2025-08-09T21:24:54Z\n",
      "\n",
      "   3. [2508.07102v1] Towards High-Order Mean Flow Generative Models: Feasibility,...\n",
      "      Authors: Yang Cao, Yubin Chen...\n",
      "      Categories: cs.LG, cs.AI, cs.CV\n",
      "      Published: 2025-08-09T21:10:58Z\n",
      "\n",
      "   4. [2508.07101v1] Less Is More: Training-Free Sparse Attention with Global Loc...\n",
      "      Authors: Lijie Yang, Zhihao Zhang...\n",
      "      Categories: cs.CL, cs.AI\n",
      "      Published: 2025-08-09T21:10:33Z\n",
      "\n",
      "   5. [2508.07095v1] Hide or Highlight: Understanding the Impact of Factuality Ex...\n",
      "      Authors: Hyo Jin Do, Werner Geyer\n",
      "      Categories: cs.HC, cs.AI\n",
      "      Published: 2025-08-09T20:45:21Z\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test Date Filtering\n",
    "async def test_date_filtering():\n",
    "    \"\"\"Test date range filtering functionality.\"\"\"\n",
    "    \n",
    "    print(\"Test 2: Date Range Filtering\")\n",
    "    \n",
    "    # Use specific dates: \n",
    "    from_date = \"20250808\"  \n",
    "    to_date = \"20250809\"    \n",
    "    try:\n",
    "        date_papers = await arxiv_client.fetch_papers(\n",
    "            max_results=5,\n",
    "            from_date=from_date,\n",
    "            to_date=to_date\n",
    "        )\n",
    "        \n",
    "        print(f\"âœ“ Date filtering test: {len(date_papers)} papers from {from_date}-{to_date}\")\n",
    "        \n",
    "        if date_papers:\n",
    "            for i, paper in enumerate(date_papers, 1):\n",
    "                print(f\"   {i}. [{paper.arxiv_id}] {paper.title[:60]}...\")\n",
    "                print(f\"      Authors: {', '.join(paper.authors[:2])}{'...' if len(paper.authors) > 2 else ''}\")\n",
    "                print(f\"      Categories: {', '.join(paper.categories)}\")\n",
    "                print(f\"      Published: {paper.published_date}\")\n",
    "                print()\n",
    "        \n",
    "        return date_papers\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âœ— Date filtering error: {e}\")\n",
    "        return []\n",
    "\n",
    "# Run date filtering test\n",
    "date_papers = await test_date_filtering()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. PDF Download and Caching\n",
    "\n",
    "Test PDF download functionality with caching:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test 3: PDF Download & Caching\n",
      "Testing PDF download for: 2508.07111v1\n",
      "Title: Investigating Intersectional Bias in Large Language Models u...\n",
      "âœ“ PDF downloaded: 2508.07111v1.pdf (6.81 MB)\n"
     ]
    }
   ],
   "source": [
    "# Test PDF Download\n",
    "async def test_pdf_download(test_papers):\n",
    "    \"\"\"Test PDF downloading with caching.\"\"\"\n",
    "\n",
    "    print(\"Test 3: PDF Download & Caching\")\n",
    "    \n",
    "    if not test_papers:\n",
    "        print(\"No papers available for PDF download test\")\n",
    "        return None\n",
    "    \n",
    "    # Test with first paper\n",
    "    test_paper = test_papers[0]\n",
    "    print(f\"Testing PDF download for: {test_paper.arxiv_id}\")\n",
    "    print(f\"Title: {test_paper.title[:60]}...\")\n",
    "    \n",
    "    try:\n",
    "        # Download PDF \n",
    "        pdf_path = await arxiv_client.download_pdf(test_paper)\n",
    "        \n",
    "        if pdf_path and pdf_path.exists():\n",
    "            size_mb = pdf_path.stat().st_size / (1024 * 1024)\n",
    "            print(f\"âœ“ PDF downloaded: {pdf_path.name} ({size_mb:.2f} MB)\")\n",
    "            \n",
    "            return pdf_path\n",
    "        else:\n",
    "            print(\"âœ— PDF download failed\")\n",
    "            return None\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"âœ— PDF download error: {e}\")\n",
    "        return None\n",
    "\n",
    "# Run PDF download test \n",
    "pdf_path = await test_pdf_download(date_papers[:1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Docling PDF Processing\n",
    "\n",
    "Test PDF parsing with Docling for structured content extraction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test 4: PDF Parsing with Docling\n",
      "========================================\n",
      "PDF parser service created\n",
      "Config: 30 pages, 20MB\n",
      "\n",
      "Found 4 PDF files to test parsing\n",
      "Testing PDF parsing with: 2508.07111v1.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter `strict_text` has been deprecated and will be ignored.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ PDF parsing successful!\n",
      "  Sections: 23\n",
      "  Raw text length: 100641 characters\n",
      "  Parser used: ParserType.DOCLING\n",
      "  First section: 'Content' (84 chars)\n"
     ]
    }
   ],
   "source": [
    "# Test PDF Parsing with Docling\n",
    "from src.services.pdf_parser.factory import make_pdf_parser_service\n",
    "from src.config import get_settings\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"Test 4: PDF Parsing with Docling\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Create PDF parser\n",
    "pdf_parser = make_pdf_parser_service()\n",
    "settings = get_settings()\n",
    "print(\"PDF parser service created\")\n",
    "print(f\"Config: {settings.pdf_parser.max_pages} pages, {settings.pdf_parser.max_file_size_mb}MB\")\n",
    "\n",
    "# Test parsing with actual PDF files\n",
    "cache_dir = Path(\"data/arxiv_pdfs\")\n",
    "if cache_dir.exists():\n",
    "    pdf_files = list(cache_dir.glob(\"*.pdf\"))\n",
    "    print(f\"\\nFound {len(pdf_files)} PDF files to test parsing\")\n",
    "    \n",
    "    if pdf_files:\n",
    "        # Test parsing the first PDF\n",
    "        test_pdf = pdf_files[0]\n",
    "        print(f\"Testing PDF parsing with: {test_pdf.name}\")\n",
    "        \n",
    "        try:\n",
    "            pdf_content = await pdf_parser.parse_pdf(test_pdf)\n",
    "            \n",
    "            if pdf_content:\n",
    "                print(f\"âœ“ PDF parsing successful!\")\n",
    "                print(f\"  Sections: {len(pdf_content.sections)}\")\n",
    "                print(f\"  Raw text length: {len(pdf_content.raw_text)} characters\")\n",
    "                print(f\"  Parser used: {pdf_content.parser_used}\")\n",
    "                \n",
    "                # Show first section as example\n",
    "                if pdf_content.sections:\n",
    "                    first_section = pdf_content.sections[0]\n",
    "                    print(f\"  First section: '{first_section.title}' ({len(first_section.content)} chars)\")\n",
    "            else:\n",
    "                print(\"âœ— PDF parsing failed (Docling compatibility issue)\")\n",
    "                print(\"This is expected - not all PDFs work with Docling\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"âœ— PDF parsing error: {e}\")\n",
    "            print(\"This demonstrates the error handling in action\")\n",
    "    else:\n",
    "        print(\"No PDF files available for parsing test\")\n",
    "else:\n",
    "    print(\"No PDF cache directory found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Database Storage Testing\n",
    "\n",
    "Test storing papers in PostgreSQL database:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test 5: Database Storage\n",
      "========================================\n",
      "âœ“ Database connection created\n",
      "Storing paper: 2601.00791v1\n",
      "âœ“ Paper stored with ID: 0bf699c9-9c39-4cd8-a71c-c30d965d0ae6\n",
      "   Database ID: 0bf699c9-9c39-4cd8-a71c-c30d965d0ae6\n",
      "   arXiv ID: 2601.00791v1\n",
      "   Title: Geometry of Reason: Spectral Signatures of Valid M...\n",
      "   Authors: 1 authors\n",
      "   Categories: cs.LG, cs.AI, cs.CL, cs.LO\n",
      "âœ“ Paper retrieval test passed\n"
     ]
    }
   ],
   "source": [
    "# Test Database Storage\n",
    "from src.db.factory import make_database\n",
    "from src.repositories.paper import PaperRepository\n",
    "from src.schemas.arxiv.paper import PaperCreate\n",
    "from dateutil import parser as date_parser\n",
    "\n",
    "print(\"Test 5: Database Storage\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Create database connection\n",
    "database = make_database()\n",
    "print(\"âœ“ Database connection created\")\n",
    "\n",
    "if papers:\n",
    "    test_paper = papers[0]\n",
    "    print(f\"Storing paper: {test_paper.arxiv_id}\")\n",
    "    \n",
    "    try:\n",
    "        with database.get_session() as session:\n",
    "            paper_repo = PaperRepository(session)\n",
    "            \n",
    "            # Convert to database format\n",
    "            published_date = date_parser.parse(test_paper.published_date) if isinstance(test_paper.published_date, str) else test_paper.published_date\n",
    "            \n",
    "            paper_create = PaperCreate(\n",
    "                arxiv_id=test_paper.arxiv_id,\n",
    "                title=test_paper.title,\n",
    "                authors=test_paper.authors,\n",
    "                abstract=test_paper.abstract,\n",
    "                categories=test_paper.categories,\n",
    "                published_date=published_date,\n",
    "                pdf_url=test_paper.pdf_url\n",
    "            )\n",
    "            \n",
    "            # Store paper (upsert to avoid duplicates)\n",
    "            stored_paper = paper_repo.upsert(paper_create)\n",
    "            \n",
    "            if stored_paper:\n",
    "                print(f\"âœ“ Paper stored with ID: {stored_paper.id}\")\n",
    "                print(f\"   Database ID: {stored_paper.id}\")\n",
    "                print(f\"   arXiv ID: {stored_paper.arxiv_id}\")\n",
    "                print(f\"   Title: {stored_paper.title[:50]}...\")\n",
    "                print(f\"   Authors: {len(stored_paper.authors)} authors\")\n",
    "                print(f\"   Categories: {', '.join(stored_paper.categories)}\")\n",
    "                \n",
    "                # Test retrieval\n",
    "                retrieved_paper = paper_repo.get_by_arxiv_id(test_paper.arxiv_id)\n",
    "                if retrieved_paper:\n",
    "                    print(f\"âœ“ Paper retrieval test passed\")\n",
    "                else:\n",
    "                    print(f\"âœ— Paper retrieval failed\")\n",
    "            else:\n",
    "                print(\"âœ— Paper storage failed\")\n",
    "                \n",
    "    except Exception as e:\n",
    "        print(f\"âœ— Database error: {e}\")\n",
    "else:\n",
    "    print(\"No papers available for database storage test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATABASE STATISTICS\n",
      "==================================================\n",
      "Total Papers: 15\n",
      "Unique Categories: 14\n",
      "Average Authors per Paper: 3.4\n",
      "Date Range: 2025-08-14 23:27:09 to 2026-01-02 18:49:37\n",
      "Parsed PDFs: 3\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# Method 4: Database Statistics\n",
    "from sqlalchemy import text\n",
    "\n",
    "with database.get_session() as session:\n",
    "    stats_query = text(\"\"\"\n",
    "        SELECT \n",
    "            COUNT(DISTINCT p.id) as total_papers,\n",
    "            COUNT(DISTINCT category) as unique_categories,\n",
    "            AVG(json_array_length(p.authors)) as avg_authors,\n",
    "            MIN(p.published_date) as earliest_paper,\n",
    "            MAX(p.published_date) as latest_paper,\n",
    "            COUNT(DISTINCT CASE WHEN p.pdf_processing_date IS NOT NULL THEN p.id END) as parsed_pdfs\n",
    "        FROM papers p\n",
    "        CROSS JOIN LATERAL json_array_elements_text(p.categories) as category\n",
    "    \"\"\")\n",
    "    \n",
    "    result = session.execute(stats_query)\n",
    "    stats = result.fetchone()\n",
    "    \n",
    "    print(\"DATABASE STATISTICS\")\n",
    "    print(\"=\" * 50)\n",
    "    if stats and stats[0] > 0:\n",
    "        print(f\"Total Papers: {stats[0]}\")\n",
    "        print(f\"Unique Categories: {stats[1]}\")\n",
    "        print(f\"Average Authors per Paper: {stats[2]:.1f}\")\n",
    "        print(f\"Date Range: {stats[3]} to {stats[4]}\")\n",
    "        print(f\"Parsed PDFs: {stats[5]}\")\n",
    "    else:\n",
    "        print(\"No papers in database yet\")\n",
    "    print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "METHOD 3: Access Database via psql CLI\n",
      "==================================================\n",
      "\n",
      "You can also access the database using psql in the Docker container:\n",
      "\n",
      "1. Open a terminal/PowerShell\n",
      "2. Run this command:\n",
      "   docker exec -it rag-postgres psql -U rag_user -d rag_db\n",
      "\n",
      "3. Then you can run SQL queries like:\n",
      "   SELECT COUNT(*) FROM papers;\n",
      "   SELECT arxiv_id, title FROM papers LIMIT 5;\n",
      "   \\dt  -- List all tables\n",
      "   \\d papers  -- Describe papers table structure\n",
      "   \\q  -- Quit psql\n",
      "\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# Method 3: Use psql in Docker (Command Line)\n",
    "print(\"METHOD 3: Access Database via psql CLI\")\n",
    "print(\"=\" * 50)\n",
    "print(\"\\nYou can also access the database using psql in the Docker container:\")\n",
    "print(\"\\n1. Open a terminal/PowerShell\")\n",
    "print(\"2. Run this command:\")\n",
    "print(\"   docker exec -it rag-postgres psql -U rag_user -d rag_db\")\n",
    "print(\"\\n3. Then you can run SQL queries like:\")\n",
    "print(\"   SELECT COUNT(*) FROM papers;\")\n",
    "print(\"   SELECT arxiv_id, title FROM papers LIMIT 5;\")\n",
    "print(\"   \\\\dt  -- List all tables\")\n",
    "print(\"   \\\\d papers  -- Describe papers table structure\")\n",
    "print(\"   \\\\q  -- Quit psql\")\n",
    "print(\"\\n\" + \"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PAPER DETAILS: 2601.00791v1\n",
      "======================================================================\n",
      "Database ID: 0bf699c9-9c39-4cd8-a71c-c30d965d0ae6\n",
      "Title: Geometry of Reason: Spectral Signatures of Valid Mathematical Reasoning\n",
      "\n",
      "Authors (1):\n",
      "  1. Valentin NoÃ«l\n",
      "\n",
      "Categories: cs.LG, cs.AI, cs.CL, cs.LO\n",
      "Published: 2026-01-02 18:49:37\n",
      "PDF URL: https://arxiv.org/pdf/2601.00791v1\n",
      "\n",
      "Abstract:\n",
      "We present a training-free method for detecting valid mathematical reasoning in large language models through spectral analysis of attention patterns. By treating attention matrices as adjacency matrices of dynamic graphs over tokens, we extract four interpretable spectral diagnostics, the Fiedler value (algebraic connectivity), high-frequency energy ratio (HFER), graph signal smoothness, and spectral entropy, that exhibit statistically significant differences between valid and invalid mathematical proofs. Experiments across seven transformer models from four independent architectural families (Meta Llama, Alibaba Qwen, Microsoft Phi, and Mistral AI) demonstrate that this spectral signature produces effect sizes up to Cohen's $d = 3.30$ ($p < 10^{-116}$), enabling 85.0--95.6\\% classification accuracy under rigorous evaluation, with calibrated thresholds reaching 93--95\\% on the full dataset. The method requires no training data, fine-tuning, or learned classifiers: a single threshold on a spectral metric suffices for high accuracy. Through systematic label correction, we discover that the spectral method detects logical coherence rather than compiler acceptance, identifying mathematically valid proofs that formal verifiers reject due to technical failures. We further identify an architectural dependency: Mistral-7B's Sliding Window Attention shifts the discriminative signal from HFER to late-layer Smoothness ($d = 2.09$, $p_{\\text{MW}} = 1.16 \\times 10^{-48}$), revealing that attention mechanism design affects which spectral features capture reasoning validity. These findings establish spectral graph analysis as a principled framework for reasoning verification with immediate applications to hallucination detection and AI safety monitoring.\n",
      "\n",
      "Metadata:\n",
      "  Created: 2026-01-05 13:47:25.174259\n",
      "  Updated: 2026-01-05 14:04:43.475834\n"
     ]
    }
   ],
   "source": [
    "# Method 2: View Detailed Paper Information\n",
    "def view_paper_details(arxiv_id: str):\n",
    "    \"\"\"View complete details of a specific paper.\"\"\"\n",
    "    with database.get_session() as session:\n",
    "        paper_repo = PaperRepository(session)\n",
    "        paper = paper_repo.get_by_arxiv_id(arxiv_id)\n",
    "        \n",
    "        if paper:\n",
    "            print(f\"PAPER DETAILS: {arxiv_id}\")\n",
    "            print(\"=\" * 70)\n",
    "            print(f\"Database ID: {paper.id}\")\n",
    "            print(f\"Title: {paper.title}\")\n",
    "            print(f\"\\nAuthors ({len(paper.authors)}):\")\n",
    "            for i, author in enumerate(paper.authors, 1):\n",
    "                print(f\"  {i}. {author}\")\n",
    "            print(f\"\\nCategories: {', '.join(paper.categories)}\")\n",
    "            print(f\"Published: {paper.published_date}\")\n",
    "            print(f\"PDF URL: {paper.pdf_url}\")\n",
    "            print(f\"\\nAbstract:\\n{paper.abstract}\")\n",
    "            print(f\"\\nMetadata:\")\n",
    "            print(f\"  Created: {paper.created_at}\")\n",
    "            print(f\"  Updated: {paper.updated_at}\")\n",
    "            if paper.pdf_processing_date:\n",
    "                print(f\"  PDF Parsed: {paper.pdf_processing_date}\")\n",
    "        else:\n",
    "            print(f\"Paper {arxiv_id} not found in database\")\n",
    "\n",
    "# Example: View the first paper we stored\n",
    "if papers:\n",
    "    view_paper_details(papers[0].arxiv_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATABASE CONTENTS VIEWER\n",
      "==================================================\n",
      "\n",
      "ðŸ“Š Total papers in database: 15\n",
      "\n",
      "ðŸ“„ Most Recent 10 Papers:\n",
      "--------------------------------------------------\n",
      "\n",
      "1. [2508.11110v1]\n",
      "   Title: Diffusion is a code repair operator and generator...\n",
      "   Authors: 4 | Categories: 3\n",
      "   Published: 2025-08-14 23:27:09 | Stored: 2026-01-05 13:55:00.797255\n",
      "\n",
      "2. [2508.11112v1]\n",
      "   Title: Quantization through Piecewise-Affine Regularization: Optimization and...\n",
      "   Authors: 2 | Categories: 4\n",
      "   Published: 2025-08-14 23:35:21 | Stored: 2026-01-05 13:55:00.735672\n",
      "\n",
      "3. [2508.11121v1]\n",
      "   Title: Tabularis Formatus: Predictive Formatting for Tables...\n",
      "   Authors: 5 | Categories: 3\n",
      "   Published: 2025-08-14 23:54:40 | Stored: 2026-01-05 13:55:00.709862\n",
      "\n",
      "4. [2601.00426v1]\n",
      "   Title: RMAAT: Astrocyte-Inspired Memory Compression and Replay for Efficient ...\n",
      "   Authors: 3 | Categories: 4\n",
      "   Published: 2026-01-01 18:34:06 | Stored: 2026-01-05 13:49:48.959193\n",
      "\n",
      "5. [2601.00448v1]\n",
      "   Title: Language as Mathematical Structure: Examining Semantic Field Theory Ag...\n",
      "   Authors: 1 | Categories: 2\n",
      "   Published: 2026-01-01 19:15:17 | Stored: 2026-01-05 13:49:48.954308\n",
      "\n",
      "6. [2601.00454v1]\n",
      "   Title: Defensive M2S: Training Guardrail Models on Compressed Multi-turn Conv...\n",
      "   Authors: 1 | Categories: 2\n",
      "   Published: 2026-01-01 19:42:08 | Stored: 2026-01-05 13:49:48.950463\n",
      "\n",
      "7. [2601.00455v1]\n",
      "   Title: Deep Networks Learn Deep Hierarchical Models...\n",
      "   Authors: 1 | Categories: 2\n",
      "   Published: 2026-01-01 19:44:53 | Stored: 2026-01-05 13:49:48.946446\n",
      "\n",
      "8. [2601.00457v1]\n",
      "   Title: Geometric Regularization in Mixture-of-Experts: The Disconnect Between...\n",
      "   Authors: 1 | Categories: 2\n",
      "   Published: 2026-01-01 19:53:01 | Stored: 2026-01-05 13:49:48.943031\n",
      "\n",
      "9. [2601.00473v1]\n",
      "   Title: Neural Chains and Discrete Dynamical Systems...\n",
      "   Authors: 3 | Categories: 2\n",
      "   Published: 2026-01-01 21:02:50 | Stored: 2026-01-05 13:49:48.939131\n",
      "\n",
      "10. [2601.00475v1]\n",
      "   Title: Progressive Ideation using an Agentic AI Framework for Human-AI Co-Cre...\n",
      "   Authors: 4 | Categories: 2\n",
      "   Published: 2026-01-01 21:06:06 | Stored: 2026-01-05 13:49:48.934103\n",
      "\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# Method 1: Query Database from Notebook\n",
    "from sqlalchemy import text\n",
    "import pandas as pd\n",
    "\n",
    "print(\"DATABASE CONTENTS VIEWER\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "with database.get_session() as session:\n",
    "    # Count total papers\n",
    "    count_result = session.execute(text(\"SELECT COUNT(*) FROM papers\"))\n",
    "    total_papers = count_result.scalar()\n",
    "    print(f\"\\nðŸ“Š Total papers in database: {total_papers}\")\n",
    "    \n",
    "    # Get recent papers\n",
    "    query = text(\"\"\"\n",
    "        SELECT \n",
    "            id,\n",
    "            arxiv_id,\n",
    "            title,\n",
    "            json_array_length(authors) as author_count,\n",
    "            json_array_length(categories) as category_count,\n",
    "            published_date,\n",
    "            created_at\n",
    "        FROM papers\n",
    "        ORDER BY created_at DESC\n",
    "        LIMIT 10\n",
    "    \"\"\")\n",
    "    \n",
    "    result = session.execute(query)\n",
    "    papers_df = pd.DataFrame(result.fetchall(), columns=result.keys())\n",
    "    \n",
    "    if not papers_df.empty:\n",
    "        print(f\"\\nðŸ“„ Most Recent {len(papers_df)} Papers:\")\n",
    "        print(\"-\" * 50)\n",
    "        for idx, row in papers_df.iterrows():\n",
    "            print(f\"\\n{idx + 1}. [{row['arxiv_id']}]\")\n",
    "            print(f\"   Title: {row['title'][:70]}...\")\n",
    "            print(f\"   Authors: {row['author_count']} | Categories: {row['category_count']}\")\n",
    "            print(f\"   Published: {row['published_date']} | Stored: {row['created_at']}\")\n",
    "    else:\n",
    "        print(\"\\nNo papers found in database\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“Š Viewing Database Contents\n",
    "\n",
    "You can view your PostgreSQL database in several ways:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test 6: Complete Metadata Fetcher Pipeline\n",
      "==================================================\n",
      "âœ“ Metadata fetcher service created\n",
      "Running small batch test (2 papers, no PDF processing for speed)...\n",
      "\n",
      "PIPELINE RESULTS:\n",
      "   Papers fetched: 2\n",
      "   PDFs downloaded: 0\n",
      "   PDFs parsed: 0\n",
      "   Papers stored: 2\n",
      "   Processing time: 0.1s\n",
      "   Errors: 0\n",
      "\n",
      "âœ“ Pipeline test successful!\n"
     ]
    }
   ],
   "source": [
    "# Test Complete Pipeline\n",
    "from src.services.metadata_fetcher import make_metadata_fetcher\n",
    "\n",
    "print(\"Test 6: Complete Metadata Fetcher Pipeline\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Create metadata fetcher\n",
    "metadata_fetcher = make_metadata_fetcher(arxiv_client, pdf_parser)\n",
    "print(\"âœ“ Metadata fetcher service created\")\n",
    "\n",
    "# Test with small batch\n",
    "print(\"Running small batch test (2 papers, no PDF processing for speed)...\")\n",
    "\n",
    "try:\n",
    "    with database.get_session() as session:\n",
    "        results = await metadata_fetcher.fetch_and_process_papers(\n",
    "            max_results=2,  \n",
    "            process_pdfs=False,  \n",
    "            store_to_db=True,\n",
    "            db_session=session\n",
    "        )\n",
    "    \n",
    "    print(\"\\nPIPELINE RESULTS:\")\n",
    "    print(f\"   Papers fetched: {results.get('papers_fetched', 0)}\")\n",
    "    print(f\"   PDFs downloaded: {results.get('pdfs_downloaded', 0)}\")\n",
    "    print(f\"   PDFs parsed: {results.get('pdfs_parsed', 0)}\")\n",
    "    print(f\"   Papers stored: {results.get('papers_stored', 0)}\")\n",
    "    print(f\"   Processing time: {results.get('processing_time', 0):.1f}s\")\n",
    "    print(f\"   Errors: {len(results.get('errors', []))}\")\n",
    "    \n",
    "    if results.get('errors'):\n",
    "        print(\"\\nErrors encountered:\")\n",
    "        for error in results.get('errors', [])[:3]:  # Show first 3 errors\n",
    "            print(f\"   - {error}\")\n",
    "    \n",
    "    if results.get('papers_fetched', 0) > 0:\n",
    "        print(\"\\nâœ“ Pipeline test successful!\")\n",
    "    else:\n",
    "        print(\"\\nNo papers fetched - may be arXiv API unavailability\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"âœ— Pipeline error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test 7: Airflow DAG Status\n",
      "========================================\n",
      "  Airflow UI Access:\n",
      "   URL: http://localhost:8080\n",
      "   Username: admin\n",
      "   Password: admin\n",
      "\n",
      "Available DAGs:\n",
      "   - arxiv_paper_ingestion: Paused\n",
      "   - hello_world_week1: Paused\n",
      "\n",
      "âœ“ No DAG import errors found\n",
      "\n",
      "  To view DAGs graphically:\n",
      "   1. Open http://localhost:8080 in your browser\n",
      "   2. Login with admin/admin\n",
      "   3. Click on 'arxiv_paper_ingestion' DAG to see the workflow\n"
     ]
    }
   ],
   "source": [
    "# Test Airflow DAGs\n",
    "print(\"Test 7: Airflow DAG Status\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "print(\"  Airflow UI Access:\")\n",
    "print(\"   URL: http://localhost:8080\")\n",
    "print(\"   Username: admin\")\n",
    "print(\"   Password: admin\")\n",
    "print()\n",
    "\n",
    "# Check DAG status using docker exec\n",
    "try:\n",
    "    result = subprocess.run(\n",
    "        [\"docker\", \"exec\", \"rag-airflow\", \"airflow\", \"dags\", \"list\"],\n",
    "        capture_output=True,\n",
    "        text=True,\n",
    "        timeout=10\n",
    "    )\n",
    "    \n",
    "    if result.returncode == 0:\n",
    "        lines = result.stdout.strip().split('\\n')\n",
    "        dag_lines = [line for line in lines if 'arxiv' in line.lower() or 'hello' in line.lower()]\n",
    "        \n",
    "        print(\"Available DAGs:\")\n",
    "        for line in dag_lines:\n",
    "            if '|' in line:\n",
    "                parts = [part.strip() for part in line.split('|')]\n",
    "                if len(parts) >= 3:\n",
    "                    dag_id = parts[0]\n",
    "                    is_paused = parts[2]\n",
    "                    status = \"Active\" if is_paused == \"False\" else \"Paused\"\n",
    "                    print(f\"   - {dag_id}: {status}\")\n",
    "        \n",
    "        # Check for import errors\n",
    "        error_result = subprocess.run(\n",
    "            [\"docker\", \"exec\", \"rag-airflow\", \"airflow\", \"dags\", \"list-import-errors\"],\n",
    "            capture_output=True,\n",
    "            text=True,\n",
    "            timeout=10\n",
    "        )\n",
    "        \n",
    "        if \"docling\" in error_result.stderr:\n",
    "            print(\"\\nKnown Issue: Docling not installed in Airflow container\")\n",
    "            print(\"   - This is expected for Week 2\")\n",
    "            print(\"   - DAG structure is complete, runtime needs container fix\")\n",
    "            print(\"   - Solution: Add docling to Airflow container startup\")\n",
    "        elif error_result.returncode == 0:\n",
    "            print(\"\\nâœ“ No DAG import errors found\")\n",
    "        \n",
    "    else:\n",
    "        print(f\"âœ— Could not list DAGs: {result.stderr}\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"âœ— Airflow test error: {e}\")\n",
    "\n",
    "print(\"\\n  To view DAGs graphically:\")\n",
    "print(\"   1. Open http://localhost:8080 in your browser\")\n",
    "print(\"   2. Login with admin/admin\")\n",
    "print(\"   3. Click on 'arxiv_paper_ingestion' DAG to see the workflow\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test 8: Complete Pipeline with PDF Processing\n",
      "==================================================\n",
      "âœ“ Using metadata fetcher service from previous test\n",
      "Running enhanced test (3 papers with PDF processing)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "d:\\Projects\\Agentic_RAG\\arxiv-paper-curator\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\91820\\.cache\\huggingface\\hub\\models--ds4sd--docling-models. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Parameter `strict_text` has been deprecated and will be ignored.\n",
      "Parameter `strict_text` has been deprecated and will be ignored.\n",
      "Parameter `strict_text` has been deprecated and will be ignored.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ENHANCED PIPELINE RESULTS:\n",
      "   Papers fetched: 3\n",
      "   PDFs downloaded: 3\n",
      "   PDFs parsed: 3\n",
      "   Papers stored: 3\n",
      "   Processing time: 189.8s\n",
      "   Errors: 0\n",
      "   Download success rate: 100.0%\n",
      "   Parse success rate: 100.0%\n",
      "\n",
      "âœ“ Enhanced pipeline test successful!\n"
     ]
    }
   ],
   "source": [
    "# Test Complete Pipeline with PDF Processing\n",
    "print(\"Test 8: Complete Pipeline with PDF Processing\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Reuse metadata fetcher from Test 6\n",
    "print(\"âœ“ Using metadata fetcher service from previous test\")\n",
    "\n",
    "# Test with small batch including PDF processing\n",
    "print(\"Running enhanced test (3 papers with PDF processing)...\")\n",
    "\n",
    "try:\n",
    "    with database.get_session() as session:\n",
    "        results = await metadata_fetcher.fetch_and_process_papers(\n",
    "            max_results=3,  # Small batch\n",
    "            from_date=\"20250813\",  # Recent date\n",
    "            to_date=\"20250814\",\n",
    "            process_pdfs=True,  \n",
    "            store_to_db=True,\n",
    "            db_session=session\n",
    "        )\n",
    "    \n",
    "    print(\"\\nENHANCED PIPELINE RESULTS:\")\n",
    "    print(f\"   Papers fetched: {results.get('papers_fetched', 0)}\")\n",
    "    print(f\"   PDFs downloaded: {results.get('pdfs_downloaded', 0)}\")\n",
    "    print(f\"   PDFs parsed: {results.get('pdfs_parsed', 0)}\")\n",
    "    print(f\"   Papers stored: {results.get('papers_stored', 0)}\")\n",
    "    print(f\"   Processing time: {results.get('processing_time', 0):.1f}s\")\n",
    "    print(f\"   Errors: {len(results.get('errors', []))}\")\n",
    "    \n",
    "    # Show success rates\n",
    "    if results.get('papers_fetched', 0) > 0:\n",
    "        download_rate = (results['pdfs_downloaded'] / results['papers_fetched']) * 100\n",
    "        parse_rate = (results['pdfs_parsed'] / results['pdfs_downloaded']) * 100 if results.get('pdfs_downloaded', 0) > 0 else 0\n",
    "        print(f\"   Download success rate: {download_rate:.1f}%\")\n",
    "        print(f\"   Parse success rate: {parse_rate:.1f}%\")\n",
    "    \n",
    "    if results.get('errors'):\n",
    "        print(\"\\nErrors encountered (showing graceful error handling):\")\n",
    "        for error in results.get('errors', [])[:3]:  # Show first 3 errors\n",
    "            print(f\"   - {error}\")\n",
    "    \n",
    "    if results.get('papers_fetched', 0) > 0:\n",
    "        print(\"\\nâœ“ Enhanced pipeline test successful!\")\n",
    "        if results.get('errors'):\n",
    "            print(\"âœ“ System continued processing despite PDF failures\")\n",
    "    else:\n",
    "        print(\"\\n! No papers fetched - may be arXiv API unavailability\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"âœ— Pipeline error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "moai-zero-to-rag (3.12.12)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

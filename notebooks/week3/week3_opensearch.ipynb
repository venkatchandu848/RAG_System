{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 3: Keyword Search First - The Critical Foundation\n",
    "\n",
    "> ** The 90% Problem:** Most RAG systems jump straight to vector search and miss the foundation that powers the best retrieval systems. We're doing it right!\n",
    "\n",
    "## ESSENTIAL SETUP - Do This First!\n",
    "\n",
    "**Before running any cells, ensure your environment is properly configured:**\n",
    "\n",
    "```bash\n",
    "# 1. CRITICAL: Copy the environment configuration\n",
    "cp .env.example .env\n",
    "\n",
    "# 2. Verify these Week 3 settings are in your .env:\n",
    "# OPENSEARCH__HOST=http://opensearch:9200\n",
    "# OPENSEARCH__INDEX_NAME=arxiv-papers\n",
    "# ARXIV__MAX_RESULTS=15\n",
    "```\n",
    "\n",
    "**Important:** Week 3 requires the `.env` file for OpenSearch connectivity and service configuration. The defaults in `.env.example` work perfectly out of the box!\n",
    "\n",
    "**Why Keyword Search First?**\n",
    "- **Exact Match Power:** Find specific technical terms and paper IDs precisely\n",
    "- **Speed & Efficiency:** BM25 is fast and doesn't require expensive embedding models\n",
    "- **Interpretable:** You understand exactly why papers were retrieved\n",
    "- **Production Reality:** Companies like Elasticsearch use keyword search as their foundation\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 3: OpenSearch Integration & BM25 Search\n",
    "\n",
    "**What We're Building This Week:**\n",
    "\n",
    "Week 3 focuses on implementing OpenSearch integration for full-text search capabilities using BM25 scoring. This transforms our system from a simple storage solution into a searchable knowledge base.\n",
    "\n",
    "## Week 3 Focus Areas\n",
    "\n",
    "### Core Objectives\n",
    "- **OpenSearch Integration**: Connect our FastAPI application to OpenSearch cluster\n",
    "- **Index Management**: Create and manage the arxiv-papers index with proper mappings\n",
    "- **BM25 Search**: Implement full-text search with relevance scoring\n",
    "- **Data Pipeline**: Transfer papers from PostgreSQL to OpenSearch\n",
    "- **Search API**: Expose search functionality through REST endpoints\n",
    "\n",
    "### What We'll Test In This Notebook\n",
    "1. **Infrastructure Verification** - Ensure all services from Week 1-2 are running\n",
    "2. **OpenSearch Service Integration** - Test client creation and health checks\n",
    "3. **Index Creation & Management** - Create arxiv-papers index with proper mappings\n",
    "4. **Data Pipeline** - Transfer papers from PostgreSQL to OpenSearch\n",
    "5. **BM25 Search Functionality** - Test search queries with relevance scoring\n",
    "6. **Search API Endpoints** - Verify FastAPI search endpoints work correctly\n",
    "\n",
    "### Success Metrics\n",
    "- OpenSearch cluster healthy and accessible\n",
    "- arxiv-papers index created with proper mappings\n",
    "- Papers successfully indexed from PostgreSQL\n",
    "- BM25 search returns relevant results with scores\n",
    "- Search API endpoints respond correctly\n",
    "- All components ready for production use\n",
    "\n",
    "---\n",
    "\n",
    "## Week 3 Component Status\n",
    "| Component | Purpose | Status |\n",
    "|-----------|---------|--------|\n",
    "| **OpenSearch Client** | Connect to OpenSearch cluster | ‚úÖ Complete |\n",
    "| **Index Management** | Create and manage search indices | ‚úÖ Complete |\n",
    "| **Query Builder** | Build complex search queries | ‚úÖ Complete |\n",
    "| **Data Pipeline** | Transfer papers to OpenSearch | ‚úÖ Complete |\n",
    "| **Search API** | REST endpoints for search | ‚úÖ Complete |\n",
    "| **BM25 Scoring** | Relevance-based search results | ‚úÖ Complete |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IMPORTANT: Week 3 Docker Services Restart\n",
    "\n",
    "**NEW USERS OR INTEGRATION CONFLICTS**: Week 3 introduces OpenSearch integration that requires fresh container state. Use this clean restart approach:\n",
    "\n",
    "### Fresh Start (Recommended for Week 3)\n",
    "```bash\n",
    "# Complete clean slate - removes all data but ensures correct OpenSearch state\n",
    "docker compose down -v\n",
    "\n",
    "# Build fresh containers with latest code\n",
    "docker compose up --build -d\n",
    "```\n",
    "\n",
    "**When to use this:**\n",
    "- First time running Week 3 \n",
    "- OpenSearch connection issues\n",
    "- Index conflicts or mapping errors\n",
    "- Want to start with clean OpenSearch state\n",
    "\n",
    "**Note**: This destroys existing data but ensures you have the correct Week 3 configuration with proper OpenSearch integration.\n",
    "\n",
    "---\n",
    "\n",
    "## Prerequisites Check\n",
    "\n",
    "**Before starting:**\n",
    "1. Week 1 infrastructure completed\n",
    "2. Week 2 arXiv integration working\n",
    "3. UV environment activated\n",
    "4. Docker Desktop running\n",
    "5. Some papers already in PostgreSQL from Week 2\n",
    "\n",
    "**Why fresh containers?** Week 3 includes OpenSearch integration that requires proper cluster initialization and may conflict with existing index states.\n",
    "\n",
    "**Service Access Points:**\n",
    "- **FastAPI**: http://localhost:8000/docs (API documentation)\n",
    "- **PostgreSQL**: via API or `docker exec -it rag-postgres psql -U rag_user -d rag_db`\n",
    "- **OpenSearch**: http://localhost:9200/_cluster/health\n",
    "- **Ollama**: http://localhost:11434 (LLM service)\n",
    "- **Airflow**: http://localhost:8080 (Username: `admin`, Password: `admin`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python Version: 3.12.12\n",
      "Environment: d:\\Projects\\Agentic_RAG\\arxiv-paper-curator\\.venv\\Scripts\\python.exe\n",
      "Project root: d:\\Projects\\Agentic_RAG\\arxiv-paper-curator\n"
     ]
    }
   ],
   "source": [
    "# Environment Setup and Path Configuration\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import json\n",
    "import requests\n",
    "\n",
    "print(f\"Python Version: {sys.version_info.major}.{sys.version_info.minor}.{sys.version_info.micro}\")\n",
    "print(f\"Environment: {sys.executable}\")\n",
    "\n",
    "# Find project root and add to Python path\n",
    "current_dir = Path.cwd()\n",
    "if current_dir.name == \"week3\" and current_dir.parent.name == \"notebooks\":\n",
    "    project_root = current_dir.parent.parent\n",
    "elif (current_dir / \"compose.yml\").exists():\n",
    "    project_root = current_dir\n",
    "else:\n",
    "    project_root = None\n",
    "\n",
    "if project_root and (project_root / \"compose.yml\").exists():\n",
    "    print(f\"Project root: {project_root}\")\n",
    "    sys.path.insert(0, str(project_root))\n",
    "else:\n",
    "    print(\"Missing compose.yml - check directory\")\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Infrastructure Verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WEEK 3 PREREQUISITE CHECK\n",
      "==================================================\n",
      "‚úì FastAPI: Healthy\n",
      "‚úì PostgreSQL (via API): Healthy\n",
      "‚úì OpenSearch: Healthy\n",
      "‚úì Airflow: Healthy\n",
      "\n",
      "All services healthy! Ready for Week 3 OpenSearch integration.\n"
     ]
    }
   ],
   "source": [
    "# Service Health Verification\n",
    "print(\"WEEK 3 PREREQUISITE CHECK\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "services_to_test = {\n",
    "    \"FastAPI\": \"http://localhost:8000/api/v1/health\",\n",
    "    \"PostgreSQL (via API)\": \"http://localhost:8000/api/v1/health\", \n",
    "    \"OpenSearch\": \"http://localhost:9200/_cluster/health\",\n",
    "    \"Airflow\": \"http://localhost:8080/health\"  \n",
    "}\n",
    "\n",
    "all_healthy = True\n",
    "\n",
    "for service_name, url in services_to_test.items():\n",
    "    try:\n",
    "        response = requests.get(url, timeout=5)\n",
    "        if response.status_code == 200:\n",
    "            print(f\"‚úì {service_name}: Healthy\")\n",
    "        else:\n",
    "            print(f\"‚úó {service_name}: HTTP {response.status_code}\")\n",
    "            all_healthy = False\n",
    "    except requests.exceptions.ConnectionError:\n",
    "        print(f\"‚úó {service_name}: Not accessible\")\n",
    "        all_healthy = False\n",
    "    except Exception as e:\n",
    "        print(f\"‚úó {service_name}: {type(e).__name__}\")\n",
    "        all_healthy = False\n",
    "\n",
    "print()\n",
    "if all_healthy:\n",
    "    print(\"All services healthy! Ready for Week 3 OpenSearch integration.\")\n",
    "else:\n",
    "    print(\"Some services need attention. Please run: docker compose up --build\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. OpenSearch Client Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OPENSEARCH CLIENT SETUP\n",
      "========================================\n",
      "Client configured with host: http://localhost:9200\n",
      "Index name: arxiv-papers\n",
      "‚úì OpenSearch health check: PASSED\n"
     ]
    }
   ],
   "source": [
    "# OpenSearch Client Setup\n",
    "from src.services.opensearch.factory import make_opensearch_client\n",
    "from opensearchpy import OpenSearch\n",
    "\n",
    "print(\"OPENSEARCH CLIENT SETUP\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Create OpenSearch client using factory pattern\n",
    "opensearch_client = make_opensearch_client()\n",
    "\n",
    "# Override for notebook execution (localhost instead of container hostname)\n",
    "opensearch_client.host = \"http://localhost:9200\"\n",
    "opensearch_client.client = OpenSearch(\n",
    "    hosts=[\"http://localhost:9200\"],\n",
    "    http_compress=True,\n",
    "    use_ssl=False,\n",
    "    verify_certs=False,\n",
    "    ssl_assert_hostname=False,\n",
    "    ssl_show_warn=False,\n",
    ")\n",
    "\n",
    "print(f\"Client configured with host: {opensearch_client.host}\")\n",
    "print(f\"Index name: {opensearch_client.index_name}\")\n",
    "\n",
    "# Test health check\n",
    "is_healthy = opensearch_client.health_check()\n",
    "if is_healthy:\n",
    "    print(\"‚úì OpenSearch health check: PASSED\")\n",
    "else:\n",
    "    print(\"‚úó OpenSearch health check: FAILED\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TESTING OPENSEARCH ENDPOINTS\n",
      "========================================\n",
      "‚úì Cluster Health Response (HTTP 200):\n",
      "{\n",
      "  \"cluster_name\": \"docker-cluster\",\n",
      "  \"status\": \"yellow\",\n",
      "  \"timed_out\": false,\n",
      "  \"number_of_nodes\": 1,\n",
      "  \"number_of_data_nodes\": 1,\n",
      "  \"discovered_master\": true,\n",
      "  \"discovered_cluster_manager\": true,\n",
      "  \"active_primary_shards\": 6,\n",
      "  \"active_shards\": 6,\n",
      "  \"relocating_shards\": 0,\n",
      "  \"initializing_shards\": 0,\n",
      "  \"unassigned_shards\": 1,\n",
      "  \"delayed_unassigned_shards\": 0,\n",
      "  \"number_of_pending_tasks\": 0,\n",
      "  \"number_of_in_flight_fetch\": 0,\n",
      "  \"task_max_waiting_in_queue_millis\": 0,\n",
      "  \"active_shards_percent_as_number\": 85.71428571428571\n",
      "}\n",
      "\n",
      "----------------------------------------\n",
      "\n",
      "‚úì Root Endpoint Response (HTTP 200):\n",
      "  Cluster: docker-cluster\n",
      "  Version: 2.19.0\n",
      "\n",
      "----------------------------------------\n",
      "\n",
      "üí° NOTE: If these work but your browser hangs:\n",
      "   ‚Ä¢ OpenSearch is working fine - it's a browser issue\n",
      "   ‚Ä¢ Try: Clear browser cache, use incognito mode\n",
      "   ‚Ä¢ Or: Use curl in terminal instead\n",
      "   ‚Ä¢ The Python client is the proper way to interact anyway!\n"
     ]
    }
   ],
   "source": [
    "# Test OpenSearch Endpoints Directly\n",
    "print(\"TESTING OPENSEARCH ENDPOINTS\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Test cluster health\n",
    "try:\n",
    "    response = requests.get(\"http://localhost:9200/_cluster/health\", timeout=5)\n",
    "    print(f\"‚úì Cluster Health Response (HTTP {response.status_code}):\")\n",
    "    print(json.dumps(response.json(), indent=2))\n",
    "except Exception as e:\n",
    "    print(f\"‚úó Error: {e}\")\n",
    "\n",
    "print(\"\\n\" + \"-\" * 40)\n",
    "\n",
    "# Test cluster info (root endpoint)\n",
    "try:\n",
    "    response = requests.get(\"http://localhost:9200/\", timeout=5)\n",
    "    print(f\"\\n‚úì Root Endpoint Response (HTTP {response.status_code}):\")\n",
    "    data = response.json()\n",
    "    print(f\"  Cluster: {data.get('cluster_name', 'N/A')}\")\n",
    "    print(f\"  Version: {data.get('version', {}).get('number', 'N/A')}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚úó Error: {e}\")\n",
    "\n",
    "print(\"\\n\" + \"-\" * 40)\n",
    "print(\"\\nüí° NOTE: If these work but your browser hangs:\")\n",
    "print(\"   ‚Ä¢ OpenSearch is working fine - it's a browser issue\")\n",
    "print(\"   ‚Ä¢ Try: Clear browser cache, use incognito mode\")\n",
    "print(\"   ‚Ä¢ Or: Use curl in terminal instead\")\n",
    "print(\"   ‚Ä¢ The Python client is the proper way to interact anyway!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Index Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INDEX CONFIGURATION\n",
      "========================================\n",
      "Index Name: arxiv-papers\n",
      "\n",
      "Key Features:\n",
      "‚Ä¢ Custom text analyzers for better search\n",
      "‚Ä¢ Multi-field mapping (text + keyword)\n",
      "‚Ä¢ 10 specialized fields for papers\n",
      "\n",
      "Field Types:\n",
      "  ‚Ä¢ arxiv_id: keyword\n",
      "  ‚Ä¢ title: text [text_analyzer]\n",
      "  ‚Ä¢ authors: text [standard_analyzer]\n",
      "  ‚Ä¢ abstract: text [text_analyzer]\n",
      "  ‚Ä¢ categories: keyword\n",
      "  ‚Ä¢ raw_text: text [text_analyzer]\n",
      "  ‚Ä¢ pdf_url: keyword\n",
      "  ‚Ä¢ published_date: date\n",
      "  ‚Ä¢ created_at: date\n",
      "  ‚Ä¢ updated_at: date\n"
     ]
    }
   ],
   "source": [
    "# Display Index Configuration\n",
    "from src.services.opensearch.index_config import ARXIV_PAPERS_INDEX, ARXIV_PAPERS_MAPPING\n",
    "\n",
    "print(\"INDEX CONFIGURATION\")\n",
    "print(\"=\" * 40)\n",
    "print(f\"Index Name: {ARXIV_PAPERS_INDEX}\")\n",
    "print(f\"\\nKey Features:\")\n",
    "print(\"‚Ä¢ Custom text analyzers for better search\")\n",
    "print(\"‚Ä¢ Multi-field mapping (text + keyword)\")\n",
    "print(\"‚Ä¢ 10 specialized fields for papers\")\n",
    "print(\"\\nField Types:\")\n",
    "\n",
    "properties = ARXIV_PAPERS_MAPPING[\"mappings\"][\"properties\"]\n",
    "for field_name, config in properties.items():\n",
    "    field_type = config.get(\"type\")\n",
    "    analyzer = config.get(\"analyzer\", \"\")\n",
    "    if analyzer:\n",
    "        print(f\"  ‚Ä¢ {field_name}: {field_type} [{analyzer}]\")\n",
    "    else:\n",
    "        print(f\"  ‚Ä¢ {field_name}: {field_type}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INDEX CREATION\n",
      "========================================\n",
      "‚úì Index 'arxiv-papers' already exists\n",
      "\n",
      "Current Statistics:\n",
      "   Documents: 0\n",
      "   Size: 208 bytes\n"
     ]
    }
   ],
   "source": [
    "# Create Index if it doesn't exist\n",
    "print(\"INDEX CREATION\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "try:\n",
    "    # Check if index already exists\n",
    "    index_exists = opensearch_client.client.indices.exists(index=opensearch_client.index_name)\n",
    "    \n",
    "    if index_exists:\n",
    "        print(f\"‚úì Index '{opensearch_client.index_name}' already exists\")\n",
    "        \n",
    "        # Get current index statistics\n",
    "        stats = opensearch_client.get_index_stats()\n",
    "        if stats and 'error' not in stats:\n",
    "            print(f\"\\nCurrent Statistics:\")\n",
    "            print(f\"   Documents: {stats.get('document_count', 0)}\")\n",
    "            print(f\"   Size: {stats.get('size_in_bytes', 0):,} bytes\")\n",
    "    else:\n",
    "        print(f\"Creating new index: {opensearch_client.index_name}\")\n",
    "        \n",
    "        # Create the index with our custom mapping\n",
    "        success = opensearch_client.create_index()\n",
    "        \n",
    "        if success:\n",
    "            print(f\"‚úì Index created successfully!\")\n",
    "        else:\n",
    "            print(f\"‚úó Index creation failed\")\n",
    "            \n",
    "except Exception as e:\n",
    "    print(f\"‚úó Error with index management: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Pipeline - Run Airflow DAG\n",
    "\n",
    "The **arxiv_paper_ingestion** DAG automatically:\n",
    "1. Fetches papers from arXiv API\n",
    "2. Stores papers in PostgreSQL\n",
    "3. **Indexes papers into OpenSearch**\n",
    "\n",
    "### Instructions:\n",
    "\n",
    "**Before proceeding, run the Airflow DAG:**\n",
    "\n",
    "1. Open Airflow UI: http://localhost:8080\n",
    "2. Login: username `admin`, password `admin`\n",
    "3. Find **`arxiv_paper_ingestion`** DAG\n",
    "4. Click the DAG name to open it\n",
    "5. Click **\"Trigger DAG\"** button (‚ñ∂Ô∏è play icon)\n",
    "6. Wait ~10 minutes for completion\n",
    "7. Check that all tasks turn green\n",
    "\n",
    "Then run the cell below to verify:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VERIFYING DATA PIPELINE\n",
      "========================================\n",
      "‚úì Success! Found 15 documents in OpenSearch\n"
     ]
    }
   ],
   "source": [
    "# Verify Data Pipeline Results\n",
    "print(\"VERIFYING DATA PIPELINE\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "stats = opensearch_client.get_index_stats()\n",
    "\n",
    "if stats and 'error' not in stats:\n",
    "    doc_count = stats.get('document_count', 0)\n",
    "    \n",
    "    if doc_count > 0:\n",
    "        print(f\"‚úì Success! Found {doc_count} documents in OpenSearch\")\n",
    "        \n",
    "        # Show sample papers\n",
    "        sample = opensearch_client.search_papers(\"*\", size=3)\n",
    "        if sample.get('hits'):\n",
    "            print(f\"\\nSample papers:\")\n",
    "            for i, paper in enumerate(sample['hits'], 1):\n",
    "                title = paper.get('title', 'Unknown')[:60]\n",
    "                print(f\"  {i}. {title}...\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è  No documents in OpenSearch yet\")\n",
    "        print(\"\\nPlease run the Airflow DAG first (see instructions above)\")\n",
    "else:\n",
    "    print(\"‚úó Could not retrieve index stats\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Simple BM25 Search\n",
    "\n",
    "Let's start with a simple search to demonstrate BM25 scoring:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SIMPLE BM25 SEARCH\n",
      "========================================\n",
      "Searching for: 'learning'\n",
      "\n",
      "Found 5 total matches\n",
      "\n",
      "1. Deep Delta Learning...\n",
      "   Score: 5.17\n",
      "   arXiv ID: 2601.00417v1\n",
      "\n",
      "2. Deep Networks Learn Deep Hierarchical Models...\n",
      "   Score: 4.32\n",
      "   arXiv ID: 2601.00455v1\n",
      "\n",
      "3. Neural Chains and Discrete Dynamical Systems...\n",
      "   Score: 3.60\n",
      "   arXiv ID: 2601.00473v1\n",
      "\n",
      "4. E-GRPO: High Entropy Steps Drive Effective Reinforcement Learning for ...\n",
      "   Score: 3.39\n",
      "   arXiv ID: 2601.00423v1\n",
      "\n",
      "5. Adaptive Causal Coordination Detection for Social Media: A Memory-Guid...\n",
      "   Score: 3.25\n",
      "   arXiv ID: 2601.00400v1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Simple BM25 Search\n",
    "print(\"SIMPLE BM25 SEARCH\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Change this to any word from your papers\n",
    "search_term = \"learning\"  # Try different terms!\n",
    "\n",
    "print(f\"Searching for: '{search_term}'\\n\")\n",
    "\n",
    "results = opensearch_client.search_papers(\n",
    "    query=search_term,\n",
    "    size=5\n",
    ")\n",
    "\n",
    "if results.get('hits'):\n",
    "    print(f\"Found {results.get('total', 0)} total matches\\n\")\n",
    "    \n",
    "    for i, paper in enumerate(results['hits'], 1):\n",
    "        print(f\"{i}. {paper.get('title', 'Unknown')[:70]}...\")\n",
    "        print(f\"   Score: {paper.get('score', 0):.2f}\")\n",
    "        print(f\"   arXiv ID: {paper.get('arxiv_id', 'N/A')}\\n\")\n",
    "else:\n",
    "    print(\"No results found. Try searching for:\")\n",
    "    print(\"  ‚Ä¢ 'neural', 'model', 'algorithm'\")\n",
    "    print(\"  ‚Ä¢ Use '*' to see all papers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Advanced OpenSearch Queries\n",
    "\n",
    "Now let's explore different query types using the OpenSearch Python client directly. This shows the power of BM25 without needing vectors!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Match Query\n",
    "\n",
    "The `match` query is the standard query for full-text search on a single field:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MATCH QUERY - Single Field Search\n",
      "========================================\n",
      "Found 4 results\n",
      "\n",
      "Title: Deep Delta Learning...\n",
      "Title: Deep Networks Learn Deep Hierarchical Models...\n",
      "Title: E-GRPO: High Entropy Steps Drive Effective Reinforcement Learning for ...\n"
     ]
    }
   ],
   "source": [
    "# Match Query - Search in title field\n",
    "print(\"MATCH QUERY - Single Field Search\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "query = {\n",
    "    \"query\": {\n",
    "        \"match\": {\n",
    "            \"title\": \"machine learning\"\n",
    "        }\n",
    "    },\n",
    "    \"size\": 3\n",
    "}\n",
    "\n",
    "response = opensearch_client.client.search(\n",
    "    index=opensearch_client.index_name,\n",
    "    body=query\n",
    ")\n",
    "\n",
    "print(f\"Found {response['hits']['total']['value']} results\\n\")\n",
    "\n",
    "for hit in response['hits']['hits']:\n",
    "    print(f\"Title: {hit['_source']['title'][:70]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Multi-Match Query\n",
    "\n",
    "Search across multiple fields simultaneously:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MULTI-MATCH QUERY - Search Multiple Fields\n",
      "========================================\n",
      "Found 6 results\n",
      "\n",
      "Title: Progressive Ideation using an Agentic AI Framework for Human-AI Co-Cre...\n",
      "Score: 9.02\n",
      "Authors: S, a...\n",
      "\n",
      "Title: Multi-Agent Coordinated Rename Refactoring...\n",
      "Score: 3.71\n",
      "Authors: A, b...\n",
      "\n",
      "Title: MAESTRO: Multi-Agent Evaluation Suite for Testing, Reliability, and Ob...\n",
      "Score: 3.11\n",
      "Authors: T, i...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Multi-Match Query - Search across multiple fields\n",
    "print(\"MULTI-MATCH QUERY - Search Multiple Fields\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "query = {\n",
    "    \"query\": {\n",
    "        \"multi_match\": {\n",
    "            \"query\": \"AI Agents\",\n",
    "            \"fields\": [\"title^2\", \"abstract\", \"authors\"],  # ^2 boosts title field\n",
    "            \"type\": \"best_fields\"\n",
    "        }\n",
    "    },\n",
    "    \"size\": 3\n",
    "}\n",
    "\n",
    "response = opensearch_client.client.search(\n",
    "    index=opensearch_client.index_name,\n",
    "    body=query\n",
    ")\n",
    "\n",
    "print(f\"Found {response['hits']['total']['value']} results\\n\")\n",
    "\n",
    "for hit in response['hits']['hits']:\n",
    "    print(f\"Title: {hit['_source']['title'][:70]}...\")\n",
    "    print(f\"Score: {hit['_score']:.2f}\")\n",
    "    print(f\"Authors: {', '.join(hit['_source']['authors'][:2])}...\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Boosting Query\n",
    "\n",
    "Boost certain results while demoting others:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BOOSTING QUERY - Promote/Demote Results\n",
      "========================================\n",
      "Query: Boost 'deep learning', demote 'survey' papers\n",
      "\n",
      "Found 5 results\n",
      "\n",
      "Title: Deep Networks Learn Deep Hierarchical Models...\n",
      "Score: 4.85\n",
      "Abstract: We consider supervised learning with $n$ labels and show that layerwise SGD on residual networks can...\n",
      "\n",
      "Title: Deep Delta Learning...\n",
      "Score: 3.66\n",
      "Abstract: The efficacy of deep residual networks is fundamentally predicated on the identity shortcut connecti...\n",
      "\n",
      "Title: Neural Chains and Discrete Dynamical Systems...\n",
      "Score: 1.80\n",
      "Abstract: We inspect the analogy between machine-learning (ML) applications based on the transformer architect...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Boosting Query - Promote and demote results\n",
    "print(\"BOOSTING QUERY - Promote/Demote Results\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "query = {\n",
    "    \"query\": {\n",
    "        \"boosting\": {\n",
    "            \"positive\": {\n",
    "                \"match\": {\n",
    "                    \"abstract\": \"deep learning\"\n",
    "                }\n",
    "            },\n",
    "            \"negative\": {\n",
    "                \"match\": {\n",
    "                    \"abstract\": \"multimodal\"\n",
    "                }\n",
    "            },\n",
    "            \"negative_boost\": 0.1  # Reduce score of negative matches\n",
    "        }\n",
    "    },\n",
    "    \"size\": 3\n",
    "}\n",
    "\n",
    "response = opensearch_client.client.search(\n",
    "    index=opensearch_client.index_name,\n",
    "    body=query\n",
    ")\n",
    "\n",
    "print(f\"Query: Boost 'deep learning', demote 'survey' papers\\n\")\n",
    "print(f\"Found {response['hits']['total']['value']} results\\n\")\n",
    "\n",
    "for hit in response['hits']['hits']:\n",
    "    title = hit['_source']['title'][:70]\n",
    "    abstract_snippet = hit['_source']['abstract'][:100]\n",
    "    print(f\"Title: {title}...\")\n",
    "    print(f\"Score: {hit['_score']:.2f}\")\n",
    "    print(f\"Abstract: {abstract_snippet}...\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4 Filter Query\n",
    "\n",
    "Filter results by specific criteria (doesn't affect scoring):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FILTER QUERY - Category Filtering\n",
      "========================================\n",
      "Found 1 results\n",
      "\n",
      "Title: Neural Chains and Discrete Dynamical Systems...\n",
      "Categories: cs.LG, cs.AI\n",
      "Score: 3.70\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Filter Query - Filter by categories\n",
    "print(\"FILTER QUERY - Category Filtering\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "query = {\n",
    "    \"query\": {\n",
    "        \"bool\": {\n",
    "            \"must\": [\n",
    "                {\n",
    "                    \"match\": {\n",
    "                        \"abstract\": \"neural\"\n",
    "                    }\n",
    "                }\n",
    "            ],\n",
    "            \"filter\": [\n",
    "                {\n",
    "                    \"terms\": {\n",
    "                        \"categories\": [\"cs.AI\"]\n",
    "                    }\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    },\n",
    "    \"size\": 3\n",
    "}\n",
    "\n",
    "response = opensearch_client.client.search(\n",
    "    index=opensearch_client.index_name,\n",
    "    body=query\n",
    ")\n",
    "\n",
    "print(f\"Found {response['hits']['total']['value']} results\\n\")\n",
    "\n",
    "for hit in response['hits']['hits']:\n",
    "    title = hit['_source']['title'][:70]\n",
    "    categories = ', '.join(hit['_source']['categories'])\n",
    "    print(f\"Title: {title}...\")\n",
    "    print(f\"Categories: {categories}\")\n",
    "    print(f\"Score: {hit['_score']:.2f}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.5 Sorting Query\n",
    "\n",
    "Sort results by different criteria:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SORTING QUERY - Latest Papers First\n",
      "========================================\n",
      "Query: All papers sorted by publication date (newest first)\n",
      "\n",
      "Date: 2026-01-01 | MotionPhysics: Learnable Motion Distillation for Text-Guided Simulatio...\n",
      "Date: 2026-01-01 | Multi-Agent Coordinated Rename Refactoring...\n",
      "Date: 2026-01-01 | MAESTRO: Multi-Agent Evaluation Suite for Testing, Reliability, and Ob...\n",
      "Date: 2026-01-01 | Progressive Ideation using an Agentic AI Framework for Human-AI Co-Cre...\n",
      "Date: 2026-01-01 | Neural Chains and Discrete Dynamical Systems...\n"
     ]
    }
   ],
   "source": [
    "# Sorting Query - Sort by publication date\n",
    "print(\"SORTING QUERY - Latest Papers First\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "query = {\n",
    "    \"query\": {\n",
    "        \"match_all\": {}  # Get all papers\n",
    "    },\n",
    "    \"sort\": [\n",
    "        {\n",
    "            \"published_date\": {\n",
    "                \"order\": \"desc\"  # Latest first\n",
    "            }\n",
    "        }\n",
    "    ],\n",
    "    \"size\": 5\n",
    "}\n",
    "\n",
    "response = opensearch_client.client.search(\n",
    "    index=opensearch_client.index_name,\n",
    "    body=query\n",
    ")\n",
    "\n",
    "print(f\"Query: All papers sorted by publication date (newest first)\\n\")\n",
    "\n",
    "for hit in response['hits']['hits']:\n",
    "    title = hit['_source']['title'][:70]\n",
    "    pub_date = hit['_source']['published_date'][:10]\n",
    "    print(f\"Date: {pub_date} | {title}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.6 Combined Query\n",
    "\n",
    "Combine multiple query types for complex searches:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COMBINED QUERY - Complex Search\n",
      "========================================\n",
      "Complex Query:\n",
      "  ‚Ä¢ Must contain 'transformer' (title boosted 3x)\n",
      "  ‚Ä¢ Filter: published after 2024-01-01\n",
      "  ‚Ä¢ Prefer: cs.AI category\n",
      "  ‚Ä¢ Sort: by relevance, then date\n",
      "\n",
      "Found 5 results\n",
      "\n",
      "Title: RMAAT: Astrocyte-Inspired Memory Compression and Replay for Efficient ...\n",
      "  Date: 2026-01-01 | Score: 6.65\n",
      "  Categories: cs.NE, cs.AI\n",
      "\n",
      "Title: Deep Delta Learning...\n",
      "  Date: 2026-01-01 | Score: 1.73\n",
      "  Categories: cs.LG, cs.AI\n",
      "\n",
      "Title: Language as Mathematical Structure: Examining Semantic Field Theory Ag...\n",
      "  Date: 2026-01-01 | Score: 1.15\n",
      "  Categories: cs.CL, cs.AI\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Combined Query - Complex search with multiple criteria\n",
    "print(\"COMBINED QUERY - Complex Search\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "query = {\n",
    "    \"query\": {\n",
    "        \"bool\": {\n",
    "            \"must\": [\n",
    "                {\n",
    "                    \"multi_match\": {\n",
    "                        \"query\": \"transformer\",\n",
    "                        \"fields\": [\"title^3\", \"abstract\"],\n",
    "                        \"type\": \"best_fields\"\n",
    "                    }\n",
    "                }\n",
    "            ],\n",
    "            \"filter\": [\n",
    "                {\n",
    "                    \"range\": {\n",
    "                        \"published_date\": {\n",
    "                            \"gte\": \"2024-01-01\"\n",
    "                        }\n",
    "                    }\n",
    "                }\n",
    "            ],\n",
    "            \"should\": [\n",
    "                {\n",
    "                    \"match\": {\n",
    "                        \"categories\": \"cs.AI\"\n",
    "                    }\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    },\n",
    "    \"sort\": [\n",
    "        \"_score\",\n",
    "        {\"published_date\": {\"order\": \"desc\"}}\n",
    "    ],\n",
    "    \"size\": 3\n",
    "}\n",
    "\n",
    "response = opensearch_client.client.search(\n",
    "    index=opensearch_client.index_name,\n",
    "    body=query\n",
    ")\n",
    "\n",
    "print(f\"Complex Query:\")\n",
    "print(f\"  ‚Ä¢ Must contain 'transformer' (title boosted 3x)\")\n",
    "print(f\"  ‚Ä¢ Filter: published after 2024-01-01\")\n",
    "print(f\"  ‚Ä¢ Prefer: cs.AI category\")\n",
    "print(f\"  ‚Ä¢ Sort: by relevance, then date\\n\")\n",
    "\n",
    "print(f\"Found {response['hits']['total']['value']} results\\n\")\n",
    "\n",
    "for hit in response['hits']['hits']:\n",
    "    title = hit['_source']['title'][:70]\n",
    "    pub_date = hit['_source']['published_date'][:10]\n",
    "    score = hit['_score']\n",
    "    categories = ', '.join(hit['_source']['categories'][:2])\n",
    "    \n",
    "    print(f\"Title: {title}...\")\n",
    "    print(f\"  Date: {pub_date} | Score: {score:.2f}\")\n",
    "    print(f\"  Categories: {categories}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### What We Demonstrated\n",
    "\n",
    "**BM25 Search is Powerful!** Without any vector embeddings, we can:\n",
    "\n",
    "1. **Simple Search**: Basic keyword search with relevance scoring\n",
    "2. **Match Queries**: Search specific fields\n",
    "3. **Multi-Match**: Search across multiple fields with boosting\n",
    "4. **Boosting**: Promote or demote certain results\n",
    "5. **Filtering**: Apply filters without affecting scores\n",
    "6. **Sorting**: Order results by date, score, or other fields\n",
    "7. **Complex Queries**: Combine all techniques for sophisticated searches\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "- **BM25 works great** for many search use cases\n",
    "- **No vectors needed** for effective full-text search\n",
    "- **Simple and fast** compared to embedding-based approaches\n",
    "- **Filters and sorting** make searches precise and relevant\n",
    "- **Field boosting** helps prioritize important content\n",
    "\n",
    "### When to Use BM25 vs Vectors\n",
    "\n",
    "**Use BM25 when:**\n",
    "- Searching for specific keywords or phrases\n",
    "- Need fast, simple implementation\n",
    "- Have good text fields with clear terminology\n",
    "- Want explainable search results\n",
    "\n",
    "**Consider vectors when:**\n",
    "- Need semantic similarity (concepts, not keywords)\n",
    "- Dealing with synonyms and paraphrasing\n",
    "- Cross-language search requirements\n",
    "- Very short queries or documents\n",
    "\n",
    "Remember: **You can also combine both** (hybrid search) for best results!\n",
    "We will see this in the next week :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "moai-zero-to-rag (3.12.12)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
